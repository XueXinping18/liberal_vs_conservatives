{"cells":[{"cell_type":"code","execution_count":null,"id":"71b12219","metadata":{"id":"71b12219"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","import torch\n","from torch import nn, optim \n","from torch.utils.data import Dataset, DataLoader\n","#from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","!pip install transformers\n","import transformers\n","from transformers import BertTokenizer, BertModel, BertConfig, AdamW\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","\n","from google.colab import drive\n","\n","# torch.manual_seed(50)\n","# np.random.seed(50)"]},{"cell_type":"markdown","id":"C3LiCHBdAqvC","metadata":{"id":"C3LiCHBdAqvC"},"source":["# New Section"]},{"cell_type":"code","source":["def data_preprocessing():\n","        drive.mount('/content/gdrive')\n","        raw_data = pd.read_csv(\"/content/gdrive/My Drive/data/liberals_vs_conservatives.csv\")\n","        raw_data.head()\n","        \n","        # We use the information of title and text to predict political lean\n","        data = raw_data[['Title', 'Text', 'Political Lean']]\n","        labels, counts = np.unique(data['Political Lean'], return_counts = True)\n","        print(labels, counts)\n","\n","        # preprocess the labels\n","        label_preprocessor = LabelEncoder()\n","        labels = torch.tensor(label_preprocessor.fit_transform(data['Political Lean'])).float()\n","        \n","        # preprocess the features\n","        features = data[['Title', 'Text']].fillna('')\n","        titles = features['Title']\n","        texts = features['Text']\n","        texts[texts != ''] = '\\n' + texts[texts != '']\n","        texts = titles + texts\n","\n","        # analyze the proportion of data to be cut\n","        lengths = texts.apply(lambda x: len(x))\n","        plt.hist(lengths, bins = 50)\n","        print(f\"the proportion to be truncated in bert is {np.sum(lengths > 128)/ len(lengths)}.\")\n","\n","        # train val test split\n","        X_train, X_test, Y_train, Y_test = train_test_split(texts.values, labels, test_size = 0.2, random_state = 50)\n","        X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size = 0.5, random_state = 50, shuffle = False)\n","        \n","        return X_train, Y_train, X_val, Y_val, X_test, Y_test\n","        \n","X_train, Y_train, X_val, Y_val, X_test, Y_test = data_preprocessing()\n"],"metadata":{"id":"ziobCS92KcQe"},"id":"ziobCS92KcQe","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"5b57c10c","metadata":{"id":"5b57c10c"},"outputs":[],"source":["# define the dataset for dataloader\n","class TokenizedDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer):\n","        super().__init__()\n","        self.texts = texts\n","        if type(labels) == torch.Tensor:\n","            self.labels = labels.float()\n","        else:\n","            self.labels = torch.Tensor(labels).float()\n","        self.tokenizer = tokenizer\n","    def __len__(self):\n","        return self.labels.shape[0]\n","    \n","    def __getitem__(self, index):\n","        one_input = self.tokenizer(self.texts[index], padding = \"max_length\", truncation = True, max_length = 128,\n","                                return_token_type_ids = True, return_tensors = 'pt')\n","        for key in one_input:\n","            one_input[key] = one_input[key].squeeze(dim = 0)\n","        one_input['labels'] = self.labels[index]\n","        return one_input\n","    \n","\n","\n","\n"]},{"cell_type":"code","source":["# dataset used for DAN\n","class IndicesDataset(Dataset):\n","    # can be used directly into the embedding layer\n","    def __init__(self, texts, labels, extractor):\n","        super().__init__()\n","        self.texts = texts\n","        if type(labels) == torch.Tensor:\n","            self.labels = labels.float()\n","        else:\n","            self.labels = torch.Tensor(labels).float()\n","        self.extractor = extractor\n","    def __len__(self):\n","        return self.labels.shape[0]\n","    \n","    def __getitem__(self, index):\n","        one_input = self.extractor.transform(self.texts[index])\n","    #    for key in one_input:\n","    #        one_input[key] = one_input[key].squeeze(dim = 0)\n","        one_input['labels'] = self.labels[index]\n","        return one_input\n","\n","# extractors\n","class FeatureExtractor(Dataset):\n","    def __init__(self, remove_stopwords = True, padding = True):\n","        self.vocab = {}\n","        # the number of words in the vocabulary\n","        self.vocab_size = 0\n","        # maximum length of sentences\n","        self.max_length = 0\n","        self.remove_stopwords = remove_stopwords\n","        self.padding = padding\n","        \n","    # lower and lemmatize the words, remove stop words\n","    def clean(self, text):      \n","        # tokenize and lower\n","        words = word_tokenize(text.lower())\n","        \n","        word_list = []\n","        \n","        lemmatizer = WordNetLemmatizer()\n","        if self.remove_stopwords:\n","            for word in words:\n","                # remove all punctuations\n","                word = ''.join((char for char in word if char not in string.punctuation))\n","                # lemmatize\n","                word = lemmatizer.lemmatize(word)\n","                # remove stopwords\n","                if (word not in stopwords.words(\"english\")) and word != '':\n","                    word_list.append(word)\n","\n","        else:\n","            for word in words:\n","                # remove all punctuations\n","                word = ''.join((char for char in word if char not in string.punctuation))\n","                # lemmatize\n","                word = lemmatizer.lemmatize(word)\n","                if word != '':\n","                    word_list.append(word)\n","        return word_list\n","    \n","    # fit the model with training data\n","    def fit(self, texts):\n","        \n","        distinct_words = set()\n","        for text in texts:\n","            word_list = self.clean(text)\n","            distinct_words.update(set(word_list))\n","            self.max_length = max(self.max_length, len(word_list))\n","        \n","        for idx, word in enumerate(list(distinct_words)):\n","            self.vocab[word] = idx\n","        self.vocab_size = len(self.vocab)\n","        \n","        # add pad token, empty token and unk token\n","        self.vocab[\"<unk>\"] = self.vocab_size\n","        self.vocab[\"<emp>\"] = self.vocab_size + 1 # Indicating the feature is empty after feature transformation\n","        if self.padding:\n","            self.vocab[\"<pad>\"] = self.vocab_size + 2\n","        # update the vocabulary size\n","        self.vocab_size = len(self.vocab)\n","        \n","    # transform text to features\n","    def transform(self, text):\n","        assert type(text) == str\n","        word_list = self.clean(text)\n","        features = []\n","        for word in word_list:\n","            index = self.vocab.get(word, self.vocab[\"<unk>\"])\n","            features.append(index)\n","        \n","        # decide whether to pad (force the same length)\n","        if self.padding: # for DAN case\n","            # when no words left, add empty token to avoid error\n","            if len(features) == 0:\n","                features.append(self.vocab[\"<emp>\"])\n","            # padding\n","            if len(features) < self.max_length:\n","                features += [self.vocab[\"<pad>\"]] * (self.max_length - len(features))\n","            \n","            features = torch.Tensor(features).long()\n","            attention_mask = (features != self.vocab[\"<pad>\"]).long()\n","            \n","            inputs = {\"input_ids\": features , \"attention_mask\": attention_mask}\n","        else: # for RNN case\n","            if len(features) == 0:\n","                    features.append(self.vocab[\"<emp>\"])\n","            features = torch.Tensor(features).long()\n","            \n","            inputs = {\"input_ids\": features}\n","        \n","        \n","        return inputs"],"metadata":{"id":"eqdUMizc_aQX"},"id":"eqdUMizc_aQX","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b3a2aaa3","metadata":{"id":"b3a2aaa3"},"outputs":[],"source":["# Define the model\n","class BertPretrainedModel(nn.Module):\n","    def __init__(self, pretrained_model_name, hidden_dim = 20, dropout_rate = 0.2):\n","        super().__init__()\n","        self.config = BertConfig(hidden_dropout_prob = dropout_rate, attention_probs_dropout_prob = dropout_rate)\n","        self.bert = BertModel.from_pretrained(pretrained_model_name)\n","        self.dropout = nn.Dropout(p = dropout_rate)\n","        self.linear1 = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n","        self.batchNorm = nn.BatchNorm1d(hidden_dim) \n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(hidden_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, inputs):\n","        assert 'attention_mask' in inputs\n","        assert 'input_ids' in inputs\n","        # pooler output is the output for each entire sequence\n","        x = self.bert(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask']).pooler_output\n","        x = self.linear1(x)\n","        x = self.batchNorm(x)\n","        x = self.relu(x)\n","        x = self.linear2(x)\n","        x = x.squeeze(dim = 1)\n","        prob = self.sigmoid(x)\n","        \n","        return prob\n","\n","\n","# Deep average network\n","class DANModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate = 0.2):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n","        self.batchNorm = nn.BatchNorm1d(hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p = dropout_rate)\n","        self.linear2 = nn.Linear(hidden_dim, 1)\n","\n","        \n","    def forward(self, inputs):\n","        \n","        x = self.embedding(inputs[\"input_ids\"])\n","        x[inputs[\"attention_mask\"]==0] = float('nan')\n","\n","        x = torch.nanmean(x, dim = -2)\n","        x = self.linear1(x)\n","        x = self.batchNorm(x)\n","        x = self.relu(x)\n","        x = self.linear2(x)\n","        x = x.squeeze(dim = 1)\n","        probs = torch.sigmoid(x)\n","\n","        return probs\n","\n","# BiLSTM network\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim = 100, hidden_dim = 20, num_layers = 1, dropout_rate = 0.2):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, \n","                            batch_first = True, bidirectional = True)\n","        self.feedforward = nn.Sequential(nn.Dropout(p = dropout_rate),\n","                                     nn.Linear(hidden_dim*num_layers*2, hidden_dim//2),\n","                                     nn.ReLU(),\n","                                     nn.Linear(hidden_dim//2, 1),\n","                                     nn.Sigmoid())\n","        \n","\n","    def forward(self, inputs):\n","        x = self.embedding(inputs[\"input_ids\"])\n","        # only final hidden states used as predictor\n","        _, (final_hidden, final_cell) = self.lstm(x)\n","        x = torch.transpose(final_hidden, 0, 1)\n","        x = x.view(x.shape[0], -1)\n","        probs = self.feedforward(x).squeeze(dim = 1)\n","\n","        return probs\n","\n","# biGRU network\n","class BiGRUModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim = 100, hidden_dim = 20, num_layers = 1, dropout_rate = 0.2):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers = num_layers, \n","                            batch_first = True, bidirectional = True)\n","        self.feedforward = nn.Sequential(nn.Dropout(p = dropout_rate),\n","                                     nn.Linear(hidden_dim*num_layers*2, hidden_dim//2),\n","                                     nn.ReLU(),\n","                                     nn.Linear(hidden_dim//2, 1),\n","                                     nn.Sigmoid())\n","        \n","\n","    def forward(self, inputs):\n","        x = self.embedding(inputs[\"input_ids\"])\n","        # only final hidden states used as predictor\n","        _, final_hidden = self.gru(x)\n","        x = torch.transpose(final_hidden, 0, 1)\n","        x = x.view(x.shape[0], -1)\n","\n","        probs = self.feedforward(x).squeeze(dim = 1)\n","\n","        return probs\n","\n","# multihead self attention model \n","# Because it does not perform well, this model is excluded from the project report\n","class BertAttentionGRUModel(nn.Module):\n","    def __init__(self, pretrained_model_name, first_hidden_dim = 200, second_hidden_dim = 100, num_heads = 4, \n","                 dropout_rate = 0.2, num_layers = 2):\n","        super().__init__()\n","        \n","        self.bert = BertModel.from_pretrained(pretrained_model_name)\n","        # build a multihead attention\n","        self.keyLinear = nn.Linear(self.bert.config.hidden_size, first_hidden_dim, bias = False)\n","        self.queryLinear = nn.Linear(self.bert.config.hidden_size, first_hidden_dim, bias = False)\n","\n","        self.multiheadAttention = nn.MultiheadAttention(first_hidden_dim, num_heads, dropout = dropout_rate, vdim = self.bert.config.hidden_size, batch_first = True)\n","#        self.gru = nn.GRU(self.bert.config.hidden_size, second_hidden_dim, batch_first = True, bidirectional = True, num_layers = num_layers)\n","#        self.gru = nn.GRU(first_hidden_dim, second_hidden_dim, batch_first = True, bidirectional = True)\n","#        self.linear = nn.Linear(second_hidden_dim * num_layers * 2, 1)\n","        self.linear = nn.Linear(first_hidden_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, inputs):\n","        assert 'attention_mask' in inputs\n","        assert 'input_ids' in inputs\n","        # we need final hidden layers of bert output at each states\n","        x = self.bert(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask']).last_hidden_state\n","\n","        # multihead attention calculations\n","        keys = self.keyLinear(x)\n","        queries = self.queryLinear(x)\n","        x, _ = self.multiheadAttention(queries, keys, x)\n","\n","        # pooling layer to choose the highest dimension (serves as nonlinearity)\n","        x = torch.transpose(x, 1, 2)\n","        x = nn.functional.max_pool1d(x, x.shape[2])\n","        x = x.squeeze(dim = 2)\n","#        _, final_hidden = self.gru(x)\n","#        x = torch.transpose(final_hidden, 0, 1)\n","#        x = x.view(x.shape[0], -1) \n","        probs = self.sigmoid(self.linear(x)).squeeze(dim = 1)\n","        \n","        return probs"]},{"cell_type":"code","execution_count":null,"id":"88c41e3e","metadata":{"id":"88c41e3e"},"outputs":[],"source":["# utility function\n","def average(L, weights):\n","    assert len(L) == len(weights) and len(L) == 2\n","    return (L[0] * weights[0] + L[1] * weights[1]) /(weights[0] + weights[1])\n","\n","# training and evaluation for each epoch\n","def train_epoch(dataloader, model, criterion, optimizer, epoch, device):\n","    model.train()\n","\n","    tqdmBar = tqdm(enumerate(dataloader), total = len(dataloader), \n","                   desc = f\"Training epoch {epoch:02d}\", leave = True)\n","    # initialize metrics\n","    metrics_book = {'n': 0, 'loss': 0, 'accuracy': 0.0}\n","    for _, inputs in tqdmBar:\n","\n","        # fetch data to gpu\n","        for key in inputs:\n","            inputs[key] = inputs[key].to(device, non_blocking = True)\n","\n","        labels = inputs['labels']\n","\n","        N = len(labels)\n","        # zero gradient\n","        \n","        optimizer.zero_grad()\n","\n","        # feedforward\n","        probs = model(inputs)\n","        preds = (probs > 0.5).float()\n","        \n","        # loss calculation and accuracy calculation\n","        loss = criterion(probs, labels)\n","        accuracy = torch.sum(preds == labels) / N\n","\n","        # backpropagation\n","        loss.backward()\n","        \n","        # update\n","        optimizer.step()\n","        \n","        # book-keeping\n","        metrics_book['loss'] = average([loss.item(), metrics_book['loss']], weights = [N, metrics_book['n']])\n","        metrics_book['accuracy'] = average([accuracy.item(), metrics_book['accuracy']], weights = [N, metrics_book['n']])\n","        metrics_book['n'] += N\n","        \n","        # add information on the bar\n","        tqdmBar.set_postfix(loss = metrics_book['loss'], accuracy = metrics_book['accuracy'])\n","\n","    return metrics_book\n","        \n","def val_epoch(dataloader, model, criterion, epoch, device):\n","    model.eval()\n","    with torch.no_grad():\n","        \n","        tqdmBar = tqdm(enumerate(dataloader), total = len(dataloader), \n","                       desc = f\" Evaluation epoch {epoch:02d}\", leave = True)\n","        metrics_book = {'n': 0, 'loss': 0, 'accuracy': 0.0}\n","        \n","        for _, inputs in tqdmBar:\n","\n","            # fetch data to gpu\n","            for key in inputs:\n","                inputs[key] = inputs[key].to(device, non_blocking = True)\n","\n","            # record information\n","            labels = inputs['labels']\n","            N = len(labels)\n","            \n","            # feedforward\n","            probs = model(inputs)\n","            preds = (probs > 0.5).int()\n","            \n","            # loss and accuracy calculation\n","            loss = criterion(probs, labels)\n","            accuracy = torch.sum(preds == labels) / N\n","            \n","            # book-keeping\n","            metrics_book['loss'] = average([loss.item(), metrics_book['loss']], weights = [N, metrics_book['n']])\n","            metrics_book['accuracy'] = average([accuracy.item(), metrics_book['accuracy']], weights = [N, metrics_book['n']])\n","            metrics_book['n'] += N\n","            \n","            # add information on the bar\n","            tqdmBar.set_postfix(loss = metrics_book['loss'], accuracy = metrics_book['accuracy'])\n","\n","    return metrics_book\n","\n","def scores(y_true, y_label):\n","    scores = {}\n","    scores['precision'] = precision_score(y_true, y_label)\n","    scores['recall'] = recall_score(y_true, y_label)\n","    scores['f1_score'] = f1_score(y_true, y_label)\n","    return scores\n","\n","def test(test_dataset, model, device):\n","    model.eval()\n","    with torch.no_grad():\n","        tqdmBar = tqdm(enumerate(test_dataset), total = len(test_dataset))\n","        preds = []\n","        labels = []\n","        for _, one_input in tqdmBar:\n","            for key in one_input:\n","                one_input[key] = one_input[key].to(device, non_blocking = True).unsqueeze(dim = 0)\n","            label = one_input['labels'].item()\n","            \n","            # feedforward\n","            prob = model(one_input).item()\n","            pred = float(int((prob > 0.5)))\n","            \n","            # record\n","            preds.append(pred)\n","            labels.append(label)\n","            # calculate all the scores\n","            score_table = scores(labels, preds)\n","\n","    return preds, labels, score_table"]},{"cell_type":"code","execution_count":null,"id":"c7bce288","metadata":{"id":"c7bce288"},"outputs":[],"source":["# Define the backbone function with the best loss being recorded\n","def train_val(train_dataloader, val_dataloader, val_dataset, model, criterion, optimizer, device, num_epochs = 10):\n","    \n","    # initialize the tracker\n","    metrics_tracker = {'train_loss':[], 'train_accuracy':[],'val_loss':[], 'val_accuracy':[]}\n","    # fetch model to gpu\n","    model = model.to(device)\n","    # main loop\n","    for epoch in range(1, num_epochs+1):\n","\n","        train_metrics = train_epoch(train_dataloader, model, criterion, optimizer, epoch, device)\n","        val_metrics = val_epoch(val_dataloader, model, criterion, epoch, device)\n","        \n","        # update the tracker\n","        metrics_tracker['train_loss'].append(train_metrics['loss'])\n","        metrics_tracker['train_accuracy'].append(train_metrics['accuracy'])\n","        metrics_tracker['val_loss'].append(val_metrics['loss'])\n","        metrics_tracker['val_accuracy'].append(val_metrics['accuracy'])\n","    \n","    # visualize the performance curve\n","    \n","    # loss curve\n","    fig1, ax1 = plt.subplots(figsize = (12, 6))\n","    ax1.set_title('Loss Curve')\n","    ax1.set_xlabel('number of epochs')\n","    ax1.set_ylabel('loss')\n","    ax1.plot(metrics_tracker['train_loss'], label = 'train', color = 'red')\n","    ax1.plot(metrics_tracker['val_loss'], label = 'val', color = 'magenta')\n","    ax1.grid()\n","    ax1.legend()\n","    plt.savefig(\"loss_curve.png\")\n","    \n","    # accuracy curve\n","    fig2, ax2 = plt.subplots(figsize = (12, 6))\n","    ax2.set_title('Accuracy Curve')\n","    ax2.set_xlabel('number of epochs')\n","    ax2.set_ylabel('accuracy')\n","    ax2.plot(metrics_tracker['train_accuracy'], label = 'train', color = 'red')\n","    ax2.plot(metrics_tracker['val_accuracy'], label = 'val', color = 'magenta')\n","    ax2.grid()\n","    ax2.legend()\n","    plt.savefig(\"accuracy_curve.png\")\n","    \n","    # obtain precision, recall, f1 scores for the final validation\n","\n","    accuracy_final = metrics_tracker['val_accuracy'][-1]\n","    _,_,score_dict = test(val_dataset, model, device)\n","    score_dict['accuracy'] = accuracy_final\n","    return score_dict"]},{"cell_type":"code","execution_count":null,"id":"160db89b","metadata":{"id":"160db89b"},"outputs":[],"source":["X_train, Y_train, X_val, Y_val, X_test, Y_test = data_preprocessing()\n","# main script\n","def run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = False, model_type = 'dan', learning_rate = 0.005, \n","        num_epochs = 20, embedding_dim = 200, hidden_dim = 20, dropout_rate = 0.2, weight_decay = 0.01,\n","        num_layers = 2, first_hidden_dim = 100, second_hidden_dim = 50, num_heads = 4):\n","    # device detect\n","    if torch.cuda.is_available():\n","        device = torch.device('cuda')\n","    else:\n","        device = torch.device('cpu')\n","    # hyperparameters irrelavent to the class\n","    # model_type = 'dan'\n","    # learning_rate = 0.005\n","    # num_epochs = 20\n","    # weight_decay = 0.01\n","    \n","    if model_type == 'bert':\n","        # hidden_dim = 50\n","        # dropout_rate = 0.2\n","        # create the model\n","        model_name = 'bert-base-uncased'\n","        model = BertPretrainedModel(model_name, hidden_dim = hidden_dim, dropout_rate = dropout_rate)\n","        # create the tokenizer\n","        bert_tokenizer  = BertTokenizer.from_pretrained(model_name)\n","        \n","        \n","        # create the dataset\n","        train_dataset = TokenizedDataset(X_train, Y_train, bert_tokenizer)\n","        val_dataset = TokenizedDataset(X_val, Y_val, bert_tokenizer)\n","        test_dataset = TokenizedDataset(X_test, Y_test, bert_tokenizer)\n","        \n","        # hidden_dim = 50\n","        # dropout_rate = 0.2\n","        # create the model\n","        model_name = 'bert-base-uncased'\n","        model = BertPretrainedModel(model_name, hidden_dim = hidden_dim, dropout_rate = dropout_rate)\n","        # create the tokenizer\n","        bert_tokenizer  = BertTokenizer.from_pretrained(model_name)\n","        \n","        \n","        # create the dataset\n","        train_dataset = TokenizedDataset(X_train, Y_train, bert_tokenizer)\n","        val_dataset = TokenizedDataset(X_val, Y_val, bert_tokenizer)\n","        test_dataset = TokenizedDataset(X_test, Y_test, bert_tokenizer)\n","\n","    elif model_type == 'dan':\n","        # create the feature extractor\n","        feature_extractor = FeatureExtractor()\n","        feature_extractor.fit(X_train)\n","        \n","        # create the model\n","        # embedding_dim = 200\n","        # hidden_dim = 50\n","        # dropout_rate = 0.2\n","        model = DANModel(feature_extractor.vocab_size, embedding_dim, hidden_dim, dropout_rate = dropout_rate)\n","        # create the dataset\n","        train_dataset = IndicesDataset(X_train, Y_train, feature_extractor)\n","        val_dataset = IndicesDataset(X_val, Y_val, feature_extractor)\n","        test_dataset = IndicesDataset(X_test, Y_test, feature_extractor)\n","\n","    elif model_type == 'lstm':\n","            \n","            # create the feature extractor\n","            feature_extractor = FeatureExtractor(padding = False, remove_stopwords = False)\n","            feature_extractor.fit(X_train)\n","            \n","            # create the model\n","            \n","            # embedding_dim = 100\n","            # hidden_dim = 20\n","            # num_layers = 2\n","            # dropout_rate = 0.2\n","            model = BiLSTMModel(feature_extractor.vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate = dropout_rate)\n","            # create the dataset\n","            train_dataset = IndicesDataset(X_train, Y_train, feature_extractor)\n","            val_dataset = IndicesDataset(X_val, Y_val, feature_extractor)\n","            test_dataset = IndicesDataset(X_test, Y_test, feature_extractor)\n","    \n","    elif model_type == 'gru':\n","            \n","        # create the feature extractor\n","        feature_extractor = FeatureExtractor(padding = False, remove_stopwords = False)\n","        feature_extractor.fit(X_train)\n","        \n","        # create the model\n","        \n","        # embedding_dim = 100\n","        # hidden_dim = 20\n","        # num_layers = 2\n","        # dropout_rate = 0.2\n","        model = BiGRUModel(feature_extractor.vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate = dropout_rate)\n","        # create the dataset\n","        train_dataset = IndicesDataset(X_train, Y_train, feature_extractor)\n","        val_dataset = IndicesDataset(X_val, Y_val, feature_extractor)\n","        test_dataset = IndicesDataset(X_test, Y_test, feature_extractor)\n","    \n","    elif model_type == 'bert_attention_gru': \n","        # first_hidden_dim = 100\n","        # second_hidden_dim = 50\n","        # num_heads = 4\n","        # num_layers = 4\n","        # dropout_rate = 0.2\n","        # create the model\n","        model_name = 'bert-base-uncased'\n","        model = BertAttentionGRUModel(model_name, first_hidden_dim, second_hidden_dim, num_heads, \n","                     dropout_rate = dropout_rate, num_layers = num_layers)\n","        # create the tokenizer\n","        bert_tokenizer  = BertTokenizer.from_pretrained(model_name)\n","        \n","        \n","        # create the dataset\n","        train_dataset = TokenizedDataset(X_train, Y_train, bert_tokenizer)\n","        val_dataset = TokenizedDataset(X_val, Y_val, bert_tokenizer)\n","        test_dataset = TokenizedDataset(X_test, Y_test, bert_tokenizer)\n","    \n","    \n","    # from dataloader to model input    \n","    if model_type in ['lstm', 'gru', 'bert_attention_gru']:\n","        batch_size = 1\n","    else:\n","        batch_size = 16\n","    train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n","    val_dataloader = DataLoader(val_dataset, shuffle = False, batch_size = batch_size)\n","\n","    # create criterion and optimizer\n","    criterion = nn.BCELoss()\n","    optimizer = optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n","\n","    # training and evaluation\n","\n","    val_scores = train_val(train_dataloader, val_dataloader, val_dataset, model, criterion, optimizer, device = device, num_epochs = num_epochs)\n","\n","    if return_model_for_test == True:\n","        preds, labels, score_table = test(test_dataset, model, device)\n","        preds = np.array(preds)\n","        labels = np.array(labels)\n","        X_test = np.array(X_test)\n","        return X_test, labels, preds, score_table, model\n","    else:\n","        return val_scores\n","\n","\n"]},{"cell_type":"code","source":["# validation for lstm\n","embedding_dim = [100, 200, 500, 1000, 200, 1000]\n","hidden_dim = [10, 30, 50, 100, 30, 100]\n","dropout_rate = [0.2, 0.2, 0.3, 0.5, 0.5, 0.2]\n","num_layers = [1,2,1,2,3,2]\n","result_lstm = {\"embedding_dim\":[], \"hidden_dim\":[], \"dropout_rate\":[], \"num_lstm_layers\":[], \"accuracy\":[], \"precision\":[] ,\"recall\":[], \"f1_score\":[]}\n","for e, h, p, l in zip(embedding_dim, hidden_dim, dropout_rate, num_layers):\n","    score_dict = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, model_type = 'lstm', num_layers = l, embedding_dim = e, hidden_dim = h, dropout_rate = p)\n","    result_lstm[\"embedding_dim\"].append(e)\n","    result_lstm[\"hidden_dim\"].append(h)\n","    result_lstm[\"dropout_rate\"].append(p)\n","    result_lstm[\"num_lstm_layers\"].append(l)\n","    for key in score_dict:\n","        result_lstm[key].append(score_dict[key])\n","        \n","pd.DataFrame(result_lstm).to_csv('/content/gdrive/My Drive/lstm_result.csv', index = False, header=True)\n","\n"],"metadata":{"id":"kDitklFypLYI"},"id":"kDitklFypLYI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"31037d09","metadata":{"id":"31037d09"},"outputs":[],"source":["\n","# validation for dan\n","embedding_dim = [100, 200, 500, 1000, 200, 1000]\n","hidden_dim = [10, 30, 50, 100, 30, 100]\n","dropout_rate = [0.2, 0.2, 0.3, 0.5, 0.5, 0.2]\n","result_dan= {\"embedding_dim\":[], \"hidden_dim\":[], \"dropout_rate\":[], \"accuracy\":[], \"precision\":[] ,\"recall\":[], \"f1_score\":[]}\n","for e, h, p in zip(embedding_dim, hidden_dim, dropout_rate):\n","    score_dict = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, model_type = 'dan', embedding_dim = e, hidden_dim = h, dropout_rate = p)\n","    result_dan[\"embedding_dim\"].append(e)\n","    result_dan[\"hidden_dim\"].append(h)\n","    result_dan[\"dropout_rate\"].append(p)\n","    for key in score_dict:\n","        result_dan[key].append(score_dict[key])\n","pd.DataFrame(result_dan).to_csv('/content/gdrive/My Drive/dan_result.csv', index = False, header=True)"]},{"cell_type":"code","source":["# validation for bert\n","num_epochs = [5, 5, 5, 5, 5, 5]\n","dropout_rate = [0.1, 0.4, 0.2, 0.4, 0.2, 0.4]\n","weight_decay = [0.01, 0.02, 0.03, 0.05, 0.1, 0.1]\n","result_bert= {\"num_epochs\":[], \"dropout_rate\":[], \"weight_decay\":[], \"accuracy\":[], \"precision\":[] ,\"recall\":[], \"f1_score\":[]}\n","for n, p, w in zip(num_epochs, dropout_rate, weight_decay):\n","    score_dict = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, model_type = 'bert', num_epochs = n, dropout_rate = p, learning_rate = 3e-5)\n","    result_bert[\"num_epochs\"].append(n)\n","    result_bert[\"dropout_rate\"].append(p)\n","    result_bert[\"weight_decay\"].append(w)\n","    for key in score_dict:\n","        result_bert[key].append(score_dict[key])\n","pd.DataFrame(result_bert).to_csv('/content/gdrive/My Drive/bert_result_weightdecay.csv', index = False, header=True)"],"metadata":{"id":"7WHxoqZ3sveW"},"id":"7WHxoqZ3sveW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# validation for gru\n","embedding_dim = [100, 200, 1000, 200, 1000]\n","hidden_dim = [10, 30, 50, 30, 100]\n","dropout_rate = [0.2, 0.2, 0.5, 0.5, 0.2]\n","num_layers = [1,2,1,2,1,2]\n","result_gru = {\"embedding_dim\":[], \"hidden_dim\":[], \"dropout_rate\":[], \"num_gru_layers\":[], \"accuracy\":[], \"precision\":[] ,\"recall\":[], \"f1_score\":[]}\n","for e, h, p, l in zip(embedding_dim, hidden_dim, dropout_rate, num_layers):\n","    score_dict = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, model_type = 'gru', num_layers = l, embedding_dim = e, hidden_dim = h, dropout_rate = p, learning_rate = 0.005)\n","    result_gru[\"embedding_dim\"].append(e)\n","    result_gru[\"hidden_dim\"].append(h)\n","    result_gru[\"dropout_rate\"].append(p)\n","    result_gru[\"num_gru_layers\"].append(l)\n","    for key in score_dict:\n","        result_gru[key].append(score_dict[key])\n","        \n","pd.DataFrame(result_gru).to_csv('/content/gdrive/My Drive/gru_result.csv', index = False, header=True)\n"],"metadata":{"id":"TeDiDwsk0QTV"},"id":"TeDiDwsk0QTV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: Because the customed model does not perform well, this part is excluded\n","# from the report\n","# validation for bert_attention_gru model\n","first_hidden_dim = [100, 150, 200, 250, 300]\n","second_hidden_dim = [10, 100, 50, 50, 30]\n","num_heads = [5,15,50,10,30]\n","dropout_rate = [0.0, 0.0, 0.0, 0.0, 0.0]\n","\n","result_bert_attention_gru = {\"first_hidden_dim\":[], \"second_hidden_dim\":[], \"num_heads\" : [],\"dropout_rate\":[], \"accuracy\":[], \"precision\":[] ,\"recall\":[], \"f1_score\":[]}\n","for f, s, n, d in zip(first_hidden_dim, second_hidden_dim, num_heads, dropout_rate):\n","    score_dict = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, model_type = 'bert_attention_gru', first_hidden_dim = 150, num_epochs = 5,\n","                     second_hidden_dim = 100, num_heads = 15, dropout_rate = 0.0, learning_rate = 3e-5)\n","    result_bert_attention_gru[\"first_hidden_dim\"].append(f)\n","    result_bert_attention_gru[\"second_hidden_dim\"].append(s)\n","    result_bert_attention_gru[\"dropout_rate\"].append(d)\n","    result_bert_attention_gru[\"num_heads\"].append(n)\n","    for key in score_dict:\n","        result_bert_attention_gru[key].append(score_dict[key])\n","        \n","pd.DataFrame(result_bert_attention_gru).to_csv('/content/gdrive/My Drive/bert_attention_gru_result.csv', index = False, header=True)\n"],"metadata":{"id":"xslhpIZFIWVb"},"id":"xslhpIZFIWVb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## examples for test scripts\n","# test script using the best F1 score obtained by each type of model\n","# X_test, labels, preds, score_table, model = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True, model_type = 'bert', learning_rate = 0.005, \n","#         num_epochs = 20, embedding_dim = 200, hidden_dim = 20, dropout_rate = 0.2, weight_decay = 0.01,\n","#         num_layers = 2, first_hidden_dim = 100, second_hidden_dim = 50, num_heads = 4)\n","# cm = confusion_matrix(labels, preds, labels = [0, 1])\n","# disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['conservatives','liberals'])\n","# disp.plot()\n","# X_wrong = X_test[labels != preds]\n","# Y_wrong = Y_test[labels != preds]\n","# nwrong = len(X_test[labels != preds])\n","# mask_wrong = np.random.choice(nwrong, size = 10, replace = False)\n","# print(X_wrong[mask_wrong])\n","# print(Y_wrong[mask_wrong])\n","# print(score_table)"],"metadata":{"id":"O71ichTBqPhU"},"id":"O71ichTBqPhU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Bert test (the best model after regularization)\n","X_test, labels, preds, score_table, model_bert = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True,\n","        model_type = 'bert', learning_rate = 3e-5, num_epochs = 5, dropout_rate = 0.2, weight_decay = 0.03)\n","cm = confusion_matrix(labels, preds, labels = [0, 1])\n","disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['conservatives','liberals'])\n","disp.plot()\n","X_wrong = X_test[labels != preds]\n","Y_wrong = Y_test[labels != preds]\n","nwrong = len(X_test[labels != preds])\n","mask_wrong = np.random.choice(nwrong, size = 10, replace = False)\n","print(X_wrong[mask_wrong])\n","print(Y_wrong[mask_wrong])\n","print(score_table)"],"metadata":{"id":"bNTJOqtOnF_x"},"id":"bNTJOqtOnF_x","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test for dan\n","X_test_, labels_, preds_, score_table_, model_ = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True,\n","        model_type = 'dan', learning_rate = 0.005, num_epochs = 20, dropout_rate = 0.2, weight_decay = 0.01, embedding_dim = 100, hidden_dim = 10)\n","cm_ = confusion_matrix(labels_, preds_, labels = [0, 1])\n","disp_ = ConfusionMatrixDisplay(confusion_matrix = cm_, display_labels = ['conservatives','liberals'])\n","disp_.plot()\n","print(score_table_)"],"metadata":{"id":"TCfnd-wKUE82"},"id":"TCfnd-wKUE82","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test for lstm\n","X_test_, labels_, preds_, score_table_, model_ = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True, num_layers = 2,\n","        model_type = 'lstm', learning_rate = 0.005, num_epochs = 20, dropout_rate = 0.5, weight_decay = 0.01, embedding_dim = 1000, hidden_dim = 100)\n","cm_ = confusion_matrix(labels_, preds_, labels = [0, 1])\n","disp_ = ConfusionMatrixDisplay(confusion_matrix = cm_, display_labels = ['conservatives','liberals'])\n","disp_.plot()\n","print(score_table_)"],"metadata":{"id":"tV70QHvqh2rY"},"id":"tV70QHvqh2rY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test for gru\n","X_test_, labels_, preds_, score_table_, model_ = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True, num_layers = 2,\n","        model_type = 'gru', learning_rate = 0.005, num_epochs = 20, dropout_rate = 0.5, weight_decay = 0.01, embedding_dim = 200, hidden_dim = 30)\n","cm_ = confusion_matrix(labels_, preds_, labels = [0, 1])\n","disp_ = ConfusionMatrixDisplay(confusion_matrix = cm_, display_labels = ['conservatives','liberals'])\n","disp_.plot()\n","print(score_table_)\n"],"metadata":{"id":"G4MbeuPlhzsb"},"id":"G4MbeuPlhzsb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Bert test \n","X_test, labels, preds, score_table, model_bert = run(X_train, Y_train, X_val, Y_val, X_test, Y_test, return_model_for_test = True,\n","        model_type = 'bert', learning_rate = 3e-5, num_epochs = 5, dropout_rate = 0.1, weight_decay = 0.01)\n","cm = confusion_matrix(labels, preds, labels = [0, 1])\n","disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['conservatives','liberals'])\n","disp.plot()\n","X_wrong = X_test[labels != preds]\n","Y_wrong = Y_test[labels != preds]\n","nwrong = len(X_test[labels != preds])\n","mask_wrong = np.random.choice(nwrong, size = 10, replace = False)\n","print(X_wrong[mask_wrong])\n","print(Y_wrong[mask_wrong])\n","print(score_table)"],"metadata":{"id":"a9ke-P6btVL0"},"id":"a9ke-P6btVL0","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"project_notebook.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}